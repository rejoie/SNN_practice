{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tanghao/miniconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import argparse, os, sys,time\n",
    "from typing import Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(T=10, b=200, data_dir='/data/tanghao/datasets/', device='cuda:0', epochs=5, j=4, lr=0.001, momentum=0.9, opt='adam', tau=2.0)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='STDP learning')\n",
    "parser.add_argument('-T', default=10, type=int, help='simulating time-steps')\n",
    "parser.add_argument('-device', default='cuda:0', help='device')\n",
    "parser.add_argument('-b', default=200, type=int, help='batch size')\n",
    "parser.add_argument('-epochs', default=5, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-j', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('-data-dir', default='/data/tanghao/datasets/', type=str, help='root dir of dataset')\n",
    "parser.add_argument('-opt', type=str, choices=['sgd', 'adam'], default='adam', help='use which optimizer. SGD or Adam')\n",
    "parser.add_argument('-momentum', default=0.9, type=float, help='momentum for SGD')\n",
    "parser.add_argument('-lr', default=1e-3, type=float, help='learning rate')\n",
    "parser.add_argument('-tau', default=2.0, type=float, help='parameter tau of LIF neuron')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=args.data_dir,\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=args.data_dir,\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_data_loader = data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=args.b,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=args.j,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_data_loader = data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=args.b,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=args.j,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset[0][0].shape)\n",
    "print(test_dataset[0][0].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Possion_Encoder(x):\n",
    "    return torch.rand_like(x).le(x).to(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "          1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "          1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "          0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "          1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(Possion_Encoder(test_dataset[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_pre(x, w_min, alpha=0.):\n",
    "    return (x - w_min) ** alpha\n",
    "\n",
    "def f_post(x, w_max, alpha=0.):\n",
    "    return (w_max - x) ** alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdp_linear_single_step(\n",
    "    fc: nn.Linear, in_spike: torch.Tensor, out_spike: torch.Tensor,\n",
    "    trace_pre: Union[float, torch.Tensor, None],\n",
    "    trace_post: Union[float, torch.Tensor, None],\n",
    "    tau_pre: float, tau_post: float, w_min: float, w_max: float,\n",
    "    f_pre: Callable = lambda x: x, f_post: Callable = lambda x: x\n",
    "):\n",
    "    if trace_pre is None:\n",
    "        trace_pre = torch.zeros_like(in_spike)\n",
    "\n",
    "    if trace_post is None:\n",
    "        trace_post = torch.zeros_like(out_spike)\n",
    "\n",
    "    weight = fc.weight.data\n",
    "    trace_pre = trace_pre - trace_pre / tau_pre + in_spike      # shape = [batch_size, N_in]\n",
    "    trace_post = trace_post - trace_post / tau_post + out_spike # shape = [batch_size, N_out]\n",
    "\n",
    "    # [batch_size, N_out, N_in] -> [N_out, N_in]\n",
    "    # 此处对照更新公式，使用unsqueeze添加更新公式中所缺失的一维\n",
    "    # print(trace_pre.shape,trace_post.shape,weight.shape,trace_pre.unsqueeze(1).shape)\n",
    "    delta_w_pre = -f_pre(weight, w_min) * (trace_post.unsqueeze(2) * in_spike.unsqueeze(1)).sum(0)\n",
    "    delta_w_post = f_post(weight, w_max) * (trace_pre.unsqueeze(1) * out_spike.unsqueeze(2)).sum(0)\n",
    "    return trace_pre, trace_post, delta_w_pre + delta_w_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处定义每个神经元的trace都是同输入维度一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_update(fc,SpikingNeuron,in_linear ,in_spike: torch.Tensor, out_spike: torch.Tensor, tau_pre: float, tau_post: float):\n",
    "    if SpikingNeuron.trace_pre is None:\n",
    "        # print('input:',in_spike)\n",
    "        SpikingNeuron.trace_pre = torch.zeros_like(in_spike)\n",
    "        fc.trace_pre = torch.zeros_like(in_spike)\n",
    "\n",
    "    if SpikingNeuron.trace_post is None:\n",
    "        SpikingNeuron.trace_post = torch.zeros_like(out_spike)\n",
    "\n",
    "    if fc.trace is None:\n",
    "        fc.trace = torch.zeros_like(in_linear)\n",
    "\n",
    "    # SpikingNeuron.trace_pre = SpikingNeuron.trace_pre - SpikingNeuron.trace_pre / \\\n",
    "    #     tau_pre + in_spike      # shape = [batch_size, N_in]\n",
    "    # SpikingNeuron.trace_post = SpikingNeuron.trace_post - \\\n",
    "    #     SpikingNeuron.trace_post / tau_post + \\\n",
    "    #     out_spike  # shape = [batch_size, N_out]\n",
    "\n",
    "    fc.trace = fc.trace - fc.trace / fc.tau + in_linear      # shape = [batch_size, N_in]\n",
    "    SpikingNeuron.trace_post = SpikingNeuron.trace_post - \\\n",
    "        SpikingNeuron.trace_post / tau_post + \\\n",
    "        out_spike  # shape = [batch_size, N_out]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdp_grad(\n",
    "    weight, in_spike: torch.Tensor, out_spike: torch.Tensor,\n",
    "    trace_pre: Union[float, torch.Tensor, None],\n",
    "    trace_post: Union[float, torch.Tensor, None],\n",
    "    w_min: float, w_max: float,\n",
    "    f_pre: Callable = lambda x: x, f_post: Callable = lambda x: x\n",
    "):#: nn.Linear\n",
    "    \n",
    "\n",
    "    # [batch_size, N_out, N_in] -> [N_out, N_in]\n",
    "    # 此处对照更新公式，使用unsqueeze添加更新公式中所缺失的一维\n",
    "    # print(trace_pre.shape,trace_post.shape,weight.shape,trace_pre.unsqueeze(1).shape)\n",
    "    # [200, 784] [200, 10] [10, 784] [200, 1, 784]\n",
    "    # torch.Size([200, 784]) torch.Size([200, 10]) torch.Size([10, 784]) torch.Size([200, 1, 784])\n",
    "    # trace_pre shape = [batch_size, N_in] = [200, 784]\n",
    "    # trace_post shape = [batch_size, N_out] = [200, 10]\n",
    "    # in_spike shape = [batch_size, N_in] = [200, 784]\n",
    "    # out_spike shape = [batch_size, N_out] = [200, 10]\n",
    "    delta_w_pre = -f_pre(weight, w_min) * (trace_post.unsqueeze(2) * in_spike.unsqueeze(1)).sum(0)\n",
    "    delta_w_post = f_post(weight, w_max) * (trace_pre.unsqueeze(1) * out_spike.unsqueeze(2)).sum(0)\n",
    "    return delta_w_pre + delta_w_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/359524837\n",
    "此处定义大于等于阈值则发射脉冲。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/winycg/article/details/100695373\n",
    "对于pytroch来说，为获取中间层的输出数值，需要使用hook函数，hook函数包括tensor的hook和nn.Module的hook。而此处为获取脉冲神经元的输入，需要得到上一层的输出，由此需要在此注册Modelu对象的hook函数。\n",
    "\n",
    "有register_forward_hook(hook)和register_backward_hook(hook)两种方法，分别对应前向传播和反向传播的hook函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook(module, input, output):\n",
    "    return output.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDP(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx,fc,SpikingNeuron,tau_pre,tau_post, w_min, w_max, f_pre, f_post, input):\n",
    "        if SpikingNeuron.v is None:\n",
    "            SpikingNeuron.v = torch.zeros_like(input)\n",
    "            SpikingNeuron.s = torch.zeros_like(input)\n",
    "        SpikingNeuron.v = SpikingNeuron.v + \\\n",
    "            (1.0 - SpikingNeuron.v) / SpikingNeuron.tau * input\n",
    "        spike = (SpikingNeuron.v >= SpikingNeuron.v_threshold).to(input)\n",
    "        if SpikingNeuron.soft_reset:\n",
    "            SpikingNeuron.v = SpikingNeuron.v-SpikingNeuron.v_threshold*spike\n",
    "        else:\n",
    "            SpikingNeuron.v = SpikingNeuron.v * \\\n",
    "                (1-spike) + SpikingNeuron.v_reset * spike\n",
    "\n",
    "        spike = SpikingNeuron.v.ge(\n",
    "            SpikingNeuron.v_threshold).to(SpikingNeuron.v)\n",
    "        \n",
    "        trace_update(SpikingNeuron, input, spike, tau_pre, tau_post)\n",
    "        ctx.save_for_backward(fc.weight.data,input, spike,SpikingNeuron.trace_pre,SpikingNeuron.trace_post)\n",
    "\n",
    "        ctx.w_min = w_min\n",
    "        ctx.w_max = w_max\n",
    "        ctx.f_pre = f_pre\n",
    "        ctx.f_post = f_post\n",
    "\n",
    "\n",
    "        return spike\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        weight,input, output,trace_pre,trace_post = ctx.saved_tensors\n",
    "        # grad_input = grad_output.clone()\n",
    "        return grad_output*stdp_grad(weight, input, output,trace_pre, trace_post, ctx.w_min, ctx.w_max, ctx.f_pre, ctx.f_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActFun(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx,in_linear ,input,fc,SpikingNeuron,tau_pre,tau_post, w_min, w_max, f_pre, f_post):\n",
    "        if SpikingNeuron.v is None:\n",
    "            SpikingNeuron.v = torch.zeros_like(input)\n",
    "            SpikingNeuron.s = torch.zeros_like(input)\n",
    "        SpikingNeuron.v = SpikingNeuron.v + (- SpikingNeuron.v+input) / SpikingNeuron.tau\n",
    "        # spike = (SpikingNeuron.v >= SpikingNeuron.v_threshold).to(input)\n",
    "        spike = SpikingNeuron.v.ge(\n",
    "            SpikingNeuron.v_threshold).to(SpikingNeuron.v)\n",
    "        if SpikingNeuron.soft_reset:\n",
    "            SpikingNeuron.v = SpikingNeuron.v-SpikingNeuron.v_threshold*spike\n",
    "        else:\n",
    "            SpikingNeuron.v = SpikingNeuron.v * \\\n",
    "                (1-spike) + SpikingNeuron.v_reset * spike\n",
    "        \n",
    "        trace_update(fc,SpikingNeuron,in_linear ,input, spike, tau_pre, tau_post)# todo: 需要考察一下trace_pre到底是linuear层的输出还是lif层的输入 fc.trace\n",
    "        ctx.save_for_backward(fc.weight.data,in_linear, spike,fc.trace,SpikingNeuron.trace_post)#SpikingNeuron.trace_pre\n",
    "\n",
    "        ctx.w_min = w_min\n",
    "        ctx.w_max = w_max\n",
    "        ctx.f_pre = f_pre\n",
    "        ctx.f_post = f_post\n",
    "\n",
    "\n",
    "        return spike\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        weight,input, output,trace_pre,trace_post = ctx.saved_tensors\n",
    "        # grad_input = grad_output.clone()\n",
    "        # grad_output.shape=[200, 10], stdp_grad.shape=[10, 784]\n",
    "        print(grad_output.shape, stdp_grad(weight, input, output,trace_pre, trace_post, ctx.w_min, ctx.w_max, ctx.f_pre, ctx.f_post).shape)\n",
    "        return grad_output*stdp_grad(weight, input, output,trace_pre, trace_post, ctx.w_min, ctx.w_max, ctx.f_pre, ctx.f_post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output=STDP.apply(fc,SpikingNeuron,tau_pre,tau_post, w_min, w_max, f_pre, f_post, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mem_update(ops,x,mem,tau,spike):\n",
    "#     mem=mem*tau*(1-spike)+ops(x)\n",
    "#     spike = act_fun(mem)\n",
    "#     return mem, spike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于STDP，当使用trace的时候，需要记录前后神经元的信息，主要需要tau的值，因此重构Linear类，加入tau属性。\n",
    "\n",
    "同时需要考察一下trace_pre到底是linuear层的输出还是lif层的输入，此处作为linear层的输出，加入该属性，方便trace_update函数的调用。\n",
    "\n",
    "在spikingjelly中，此部分代码为：\n",
    "```python\n",
    "class STDPLearner(base.MemoryModule):\n",
    "    def __init__(\n",
    "        self, step_mode: str,\n",
    "        synapse: Union[nn.Conv2d, nn.Linear], sn: neuron.BaseNode,\n",
    "        ...\n",
    "        self.in_spike_monitor = monitor.InputMonitor(synapse)\n",
    "        self.out_spike_monitor = monitor.OutputMonitor(sn)\n",
    "\n",
    "def stdp_linear_single_step(\n",
    "    fc: nn.Linear, in_spike: torch.Tensor, out_spike: torch.Tensor,\n",
    "    trace_pre: Union[float, torch.Tensor, None],\n",
    "    trace_post: Union[float, torch.Tensor, None],\n",
    "    ...\n",
    "    trace_pre = trace_pre - trace_pre / tau_pre + in_spike      # shape = [batch_size, N_in]\n",
    "    trace_post = trace_post - trace_post / tau_post + out_spike # shape = [batch_size, N_out]\n",
    "```\n",
    "因此他认为，trace_pre是linear层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True,tau=2.):\n",
    "        super(Linear, self).__init__(in_features, out_features, bias)\n",
    "        self.tau = tau\n",
    "        self.trace = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.trace = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LIFNeuron(nn.Module):\n",
    "#     def __init__(self ,weight,tau=2.0,tau_pre=2.0, v_threshold=1.0, v_reset=0.0, soft_reset=True):\n",
    "#         super(LIFNeuron, self).__init__()\n",
    "#         self.tau = tau\n",
    "#         self.tau_pre = tau_pre\n",
    "#         self.v = None\n",
    "#         self.s = None\n",
    "#         self.v_threshold = v_threshold\n",
    "#         self.v_reset = v_reset\n",
    "#         self.soft_reset = soft_reset\n",
    "#         self.trace_pre = None\n",
    "#         self.trace_post = None\n",
    "        \n",
    "\n",
    "#     def reset(self):\n",
    "#         self.v = None\n",
    "#         self.s = None\n",
    "#         self.trace_pre = None\n",
    "#         self.trace_post = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         spike=STDP.apply(x,self)\n",
    "\n",
    "#         return spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LIFNeuron(nn.Module):\n",
    "#     def __init__(self, tau=2.0, v_threshold=1.0, v_reset=0.0, soft_reset=True):\n",
    "#         super(LIFNeuron, self).__init__()\n",
    "#         self.tau = tau\n",
    "#         self.v = None\n",
    "#         self.s = None\n",
    "#         self.v_threshold = v_threshold\n",
    "#         self.v_reset = v_reset\n",
    "#         self.soft_reset = soft_reset\n",
    "#         self.trace_pre = None\n",
    "#         self.trace_post = None\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.v = None\n",
    "#         self.s = None\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         if self.v is None:\n",
    "#             self.v = torch.zeros_like(x)\n",
    "#             self.s = torch.zeros_like(x)\n",
    "#         self.v = self.v + (1.0 - self.v) / self.tau * x\n",
    "#         spike = (self.v >= self.v_threshold).to(x)\n",
    "#         if self.soft_reset:\n",
    "#             self.v = self.v-self.v_threshold*spike\n",
    "#         else:\n",
    "#             self.v = self.v * (1-spike) + self.v_reset * spike\n",
    "\n",
    "#         return (self.v >= self.v_threshold).to(self.v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当使用`loss.backward()`时，需要计算全连接层权重的梯度，即计算\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w}=\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial w}\n",
    "$$\n",
    "其中，$z$为LIF模型输出，$y$为全连接层输出。\n",
    "$\\frac{\\partial L}{\\partial z}$为自定义反向传播的grad_output，自定义的反向传播值为$\\frac{\\partial y}{\\partial z}$.\n",
    "\n",
    "对于nn.Linear来说，$y=X\\cdot W^T$，其中，$X$为$m\\times n$的矩阵，$W$为$p\\times n$的矩阵，$y$为$m\\times p$的矩阵。\n",
    "\n",
    "则$\\frac{\\partial y}{\\partial w}$为$m\\times p$的矩阵，其每个元素为$\\frac{\\partial y_{ij}}{\\partial w_{kl}}$，其中，$i$为行，$j$为列，$k$为行，$l$为列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Spiking(nn.Module):\n",
    "    def __init__(self, in_feature, out_feature, f_pre, f_post, tau=2.0, tau_pre=2.0, v_threshold=1.0, v_reset=0.0, soft_reset=True, w_min=-1.0, w_max=1.0):\n",
    "        super(Linear_Spiking, self).__init__()\n",
    "        self.tau = tau\n",
    "        self.v = None\n",
    "        self.s = None\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.soft_reset = soft_reset\n",
    "        self.trace_pre = None\n",
    "        self.trace_post = None\n",
    "        self.w_min = w_min\n",
    "        self.w_max = w_max\n",
    "        self.f_pre = f_pre\n",
    "        self.f_post = f_post\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = Linear(in_feature, out_feature, bias=False, tau=tau_pre)\n",
    "        # self.lif=LIFNeuron(tau=tau, v_threshold=v_threshold, v_reset=v_reset, soft_reset=soft_reset)\n",
    "\n",
    "    def reset(self):\n",
    "        self.linear.reset()\n",
    "        self.v = None\n",
    "        self.s = None\n",
    "        self.trace_pre = None\n",
    "        self.trace_post = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.flatten(x)\n",
    "        self.linear.weight.data.clamp_(self.w_min, self.w_max)\n",
    "        in_spike = self.linear(y)\n",
    "        # print(input.shape)\n",
    "        # if self.v is None:\n",
    "        #     self.v = torch.zeros_like(input)\n",
    "        #     self.s = torch.zeros_like(input)\n",
    "        # self.v = self.v + (1.0 - self.v) / self.tau * input\n",
    "        # spike = (self.v >= self.v_threshold).to(input)\n",
    "        # if self.soft_reset:\n",
    "        #     self.v = self.v-self.v_threshold*spike\n",
    "        # else:\n",
    "        #     self.v = self.v * (1-spike) + self.v_reset * spike\n",
    "        # (ctx, input,fc,SpikingNeuron,tau_pre,tau_post, w_min, w_max, f_pre, f_post)\n",
    "        spike = ActFun.apply(y,in_spike, self.linear, self, self.linear.tau,\n",
    "                             self.tau, self.w_min, self.w_max, self.f_pre, self.f_post)\n",
    "\n",
    "        return spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Linear_Spiking(nn.Module):\n",
    "#     def __init__(self, N_in, N_out, tau=2.0, v_threshold=1.0, v_reset=0.0, soft_reset=True):\n",
    "#         super(Linear_Spiking, self).__init__()\n",
    "#         self.lif = LIFNeuron(tau=tau, v_threshold=v_threshold,\n",
    "#                              v_reset=v_reset, soft_reset=soft_reset)\n",
    "#         self.net = nn.Sequential(\n",
    "#             # nn.Flatten(),\n",
    "#             Linear(N_in, N_out, bias=False,tau=2.),\n",
    "#             LIFNeuron(tau=tau, v_threshold=v_threshold,\n",
    "#                       v_reset=v_reset, soft_reset=soft_reset)\n",
    "#         )\n",
    "#         # self.handle=self.net[0].register_forward_hook(hook)\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.lif.reset()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y = self.net(x)\n",
    "#         # if on_grad:\n",
    "#         #     if self.synapse.weight.grad is None:\n",
    "#         #         self.synapse.weight.grad = -delta_w\n",
    "#         #     else:\n",
    "#         #         self.synapse.weight.grad = self.synapse.weight.grad - delta_w\n",
    "#         # else:\n",
    "#         #     return delta_w\n",
    "#         return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Linear_Spiking(784,10,f_pre, f_post)\n",
    "# if args.opt == 'adam':\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "model=model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    }
   ],
   "source": [
    "for item in model.parameters():\n",
    "    print(item.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7171, 0.5731, 0.0279,  ..., 0.9072, 0.8416, 0.5684],\n",
      "        [0.3181, 0.1979, 0.6324,  ..., 0.5656, 0.8090, 0.6182],\n",
      "        [0.2453, 0.8775, 0.4082,  ..., 0.4226, 0.4794, 0.9095],\n",
      "        ...,\n",
      "        [0.7169, 0.0875, 0.5168,  ..., 0.3958, 0.0653, 0.9119],\n",
      "        [0.1037, 0.6083, 0.0148,  ..., 0.5965, 0.9038, 0.9610],\n",
      "        [0.4198, 0.6720, 0.6086,  ..., 0.4907, 0.4397, 0.3188]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0137, -0.0128,  0.0069,  ..., -0.0109, -0.0007,  0.0099],\n",
      "        [-0.0193, -0.0287, -0.0151,  ..., -0.0254, -0.0139, -0.0271],\n",
      "        [-0.0329,  0.0083,  0.0298,  ..., -0.0232, -0.0212,  0.0169],\n",
      "        ...,\n",
      "        [-0.0099, -0.0263,  0.0098,  ..., -0.0223,  0.0094, -0.0254],\n",
      "        [-0.0216,  0.0084, -0.0029,  ..., -0.0162, -0.0305,  0.0315],\n",
      "        [-0.0056, -0.0263,  0.0056,  ..., -0.0016,  0.0349, -0.0276]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0137, -0.0128,  0.0069,  ..., -0.0109, -0.0007,  0.0099],\n",
      "        [-0.0193, -0.0287, -0.0151,  ..., -0.0254, -0.0139, -0.0271],\n",
      "        [-0.0329,  0.0083,  0.0298,  ..., -0.0232, -0.0212,  0.0169],\n",
      "        ...,\n",
      "        [-0.0099, -0.0263,  0.0098,  ..., -0.0223,  0.0094, -0.0254],\n",
      "        [-0.0216,  0.0084, -0.0029,  ..., -0.0162, -0.0305,  0.0315],\n",
      "        [-0.0056, -0.0263,  0.0056,  ..., -0.0016,  0.0349, -0.0276]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "model.linear.weight.grad=torch.rand_like(model.linear.weight)\n",
    "print(model.linear.weight.grad)\n",
    "print(model.linear.weight)\n",
    "model.linear.weight.backward(torch.ones_like(model.linear.weight))\n",
    "print(model.linear.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 10]) torch.Size([10, 784])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (784) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmse_loss(out_fr, label_onehot)\n\u001b[1;32m     25\u001b[0m \u001b[39m# model.net[1].trace_pre, model.net[1].trece_post, model.net[0].weight.grad = stdp_linear_single_step(\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m#     model.net[0], imag_possion, out_fr, model.net[1].trace_pre, model.net[1].trace_post, 1, 1, 0, 1, f_pre, f_post)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     28\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m train_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mnumel()\n",
      "File \u001b[0;32m/data/tanghao/miniconda3/envs/py38/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/data/tanghao/miniconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/data/tanghao/miniconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/function.py:199\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mImplementing both \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbackward\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39mvjp\u001b[39m\u001b[39m'\u001b[39m\u001b[39m for a custom \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mFunction is not allowed. You should only implement one \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mof them.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    198\u001b[0m user_fn \u001b[39m=\u001b[39m vjp_fn \u001b[39mif\u001b[39;00m vjp_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Function\u001b[39m.\u001b[39mvjp \u001b[39melse\u001b[39;00m backward_fn\n\u001b[0;32m--> 199\u001b[0m \u001b[39mreturn\u001b[39;00m user_fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs)\n",
      "Cell \u001b[0;32mIn [13], line 35\u001b[0m, in \u001b[0;36mActFun.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m# grad_input = grad_output.clone()\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[39m# grad_output.shape=[200, 10], stdp_grad.shape=[10, 784]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39mprint\u001b[39m(grad_output\u001b[39m.\u001b[39mshape, stdp_grad(weight, \u001b[39minput\u001b[39m, output,trace_pre, trace_post, ctx\u001b[39m.\u001b[39mw_min, ctx\u001b[39m.\u001b[39mw_max, ctx\u001b[39m.\u001b[39mf_pre, ctx\u001b[39m.\u001b[39mf_post)\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 35\u001b[0m \u001b[39mreturn\u001b[39;00m grad_output\u001b[39m*\u001b[39;49mstdp_grad(weight, \u001b[39minput\u001b[39;49m, output,trace_pre, trace_post, ctx\u001b[39m.\u001b[39;49mw_min, ctx\u001b[39m.\u001b[39;49mw_max, ctx\u001b[39m.\u001b[39;49mf_pre, ctx\u001b[39m.\u001b[39;49mf_post)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (784) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "acc_max = 0\n",
    "epoch_max = 0\n",
    "for epoch in range(args.epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    model.train()\n",
    "    for imag, label in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        imag = imag.to(args.device)\n",
    "        label = label.to(args.device)\n",
    "        label_onehot = F.one_hot(label, 10).float().to(args.device)\n",
    "        out_fr = 0.\n",
    "        for t in range(args.T):\n",
    "            imag_possion = Possion_Encoder(imag)\n",
    "            out_fr += model(imag_possion)\n",
    "            # model.net[1].trace_pre, model.net[1].trece_post, grad = stdp_linear_single_step(\n",
    "            #     model.net[0], imag_possion, out_fr, model.net[1].trace_pre, model.net[1].trace_post, 1, 1, 0, 1, f_pre, f_post)\n",
    "            # model.net[0].weight.grad += grad\n",
    "\n",
    "        out_fr = out_fr / args.T\n",
    "\n",
    "        loss = F.mse_loss(out_fr, label_onehot)\n",
    "        # model.net[1].trace_pre, model.net[1].trece_post, model.net[0].weight.grad = stdp_linear_single_step(\n",
    "        #     model.net[0], imag_possion, out_fr, model.net[1].trace_pre, model.net[1].trace_post, 1, 1, 0, 1, f_pre, f_post)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "    print('Epoch: {}, Train Loss: {:.4f}, Train Acc: {:.4f}, Time: {:.4f}'.format(\n",
    "        epoch, train_loss / train_samples, train_acc / train_samples, time.time() - start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51582ed516be4194d0681ab41331bdf49015ae552f1f95522fcb6a8efc2a378b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

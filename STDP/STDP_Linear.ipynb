{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/tanghao/miniconda3/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import argparse, os, sys,time\n",
    "from typing import Callable, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(T=100, b=200, data_dir='/data/tanghao/datasets/', device='cuda:0', epochs=5, j=4, lr=0.001, momentum=0.9, opt='adam', tau=2.0)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='STDP learning')\n",
    "parser.add_argument('-T', default=100, type=int, help='simulating time-steps')\n",
    "parser.add_argument('-device', default='cuda:0', help='device')\n",
    "parser.add_argument('-b', default=200, type=int, help='batch size')\n",
    "parser.add_argument('-epochs', default=5, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-j', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('-data-dir', default='/data/tanghao/datasets/', type=str, help='root dir of dataset')\n",
    "parser.add_argument('-opt', type=str, choices=['sgd', 'adam'], default='adam', help='use which optimizer. SGD or Adam')\n",
    "parser.add_argument('-momentum', default=0.9, type=float, help='momentum for SGD')\n",
    "parser.add_argument('-lr', default=1e-3, type=float, help='learning rate')\n",
    "parser.add_argument('-tau', default=2.0, type=float, help='parameter tau of LIF neuron')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=args.data_dir,\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=args.data_dir,\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_data_loader = data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=args.b,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=args.j,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_data_loader = data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=args.b,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=args.j,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Possion_Encoder(x):\n",
    "    return torch.rand_like(x).le(x).to(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_pre(x, w_min, alpha=0.):\n",
    "    return (x - w_min) ** alpha\n",
    "\n",
    "def f_post(x, w_max, alpha=0.):\n",
    "    return (w_max - x) ** alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处定义每个神经元的trace都是同输入维度一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_update(SpikingNeuron,in_spike: torch.Tensor, out_spike: torch.Tensor, tau_pre: float, tau_post: float):\n",
    "    if SpikingNeuron.trace_pre is None:\n",
    "        # print('input:',in_spike)\n",
    "        SpikingNeuron.trace_pre = torch.zeros_like(in_spike)\n",
    "\n",
    "    if SpikingNeuron.trace_post is None:\n",
    "        SpikingNeuron.trace_post = torch.zeros_like(out_spike)\n",
    "\n",
    "\n",
    "    # SpikingNeuron.trace_pre = SpikingNeuron.trace_pre - SpikingNeuron.trace_pre / \\\n",
    "    #     tau_pre + in_spike      # shape = [batch_size, N_in]\n",
    "    # SpikingNeuron.trace_post = SpikingNeuron.trace_post - \\\n",
    "    #     SpikingNeuron.trace_post / tau_post + \\\n",
    "    #     out_spike  # shape = [batch_size, N_out]\n",
    "\n",
    "    SpikingNeuron.trace_pre = SpikingNeuron.trace_pre - SpikingNeuron.trace_pre / tau_pre + in_spike      # shape = [batch_size, N_in]\n",
    "    SpikingNeuron.trace_post = SpikingNeuron.trace_post - SpikingNeuron.trace_post / tau_post + out_spike  # shape = [batch_size, N_out]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdp_grad(\n",
    "    weight, in_spike: torch.Tensor, out_spike: torch.Tensor,\n",
    "    trace_pre: Union[float, torch.Tensor, None],\n",
    "    trace_post: Union[float, torch.Tensor, None],\n",
    "    w_min: float, w_max: float,\n",
    "    f_pre: Callable = lambda x: x, f_post: Callable = lambda x: x\n",
    "):#: nn.Linear\n",
    "    \n",
    "\n",
    "    # [batch_size, N_out, N_in] -> [N_out, N_in]\n",
    "    # 此处对照更新公式，使用unsqueeze添加更新公式中所缺失的一维\n",
    "    # print(trace_pre.shape,trace_post.shape,weight.shape,trace_pre.unsqueeze(1).shape)\n",
    "    # [200, 784] [200, 10] [10, 784] [200, 1, 784]\n",
    "    # torch.Size([200, 784]) torch.Size([200, 10]) torch.Size([10, 784]) torch.Size([200, 1, 784])\n",
    "    # trace_pre shape = [batch_size, N_in] = [200, 784]\n",
    "    # trace_post shape = [batch_size, N_out] = [200, 10]\n",
    "    # in_spike shape = [batch_size, N_in] = [200, 784]\n",
    "    # out_spike shape = [batch_size, N_out] = [200, 10]\n",
    "    delta_w_pre = -f_pre(weight, w_min) * (trace_post.unsqueeze(2) * in_spike.unsqueeze(1)).sum(0)\n",
    "    delta_w_post = f_post(weight, w_max) * (trace_pre.unsqueeze(1) * out_spike.unsqueeze(2)).sum(0)\n",
    "    return delta_w_pre + delta_w_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/359524837\n",
    "此处定义大于等于阈值则发射脉冲。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/winycg/article/details/100695373\n",
    "对于pytroch来说，为获取中间层的输出数值，需要使用hook函数，hook函数包括tensor的hook和nn.Module的hook。而此处为获取脉冲神经元的输入，需要得到上一层的输出，由此需要在此注册Modelu对象的hook函数。\n",
    "\n",
    "有register_forward_hook(hook)和register_backward_hook(hook)两种方法，分别对应前向传播和反向传播的hook函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActFun(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx,input,fc,SpikingNeuron,tau_pre,tau_post, w_min, w_max, f_pre, f_post):\n",
    "        in_spike=fc(input)\n",
    "        if SpikingNeuron.v is None:\n",
    "            SpikingNeuron.v = torch.zeros_like(in_spike)\n",
    "            SpikingNeuron.s = torch.zeros_like(in_spike)\n",
    "\n",
    "        SpikingNeuron.v = SpikingNeuron.v + (-SpikingNeuron.v+in_spike) / tau_post# todo: 此处的tau\n",
    "        spike = SpikingNeuron.v.ge(SpikingNeuron.v_threshold).to(SpikingNeuron.v)\n",
    "        # SpikingNeuron.v = SpikingNeuron.v + \\\n",
    "        #     (1.0 - SpikingNeuron.s) / tau_post * in_spike# todo: 此处的tau\n",
    "        # spike = (SpikingNeuron.v >= SpikingNeuron.v_threshold).to(in_spike)\n",
    "        if SpikingNeuron.soft_reset:\n",
    "            SpikingNeuron.v = SpikingNeuron.v-SpikingNeuron.v_threshold*spike\n",
    "        else:\n",
    "            SpikingNeuron.v = SpikingNeuron.v * (1-spike) + SpikingNeuron.v_reset * spike\n",
    "\n",
    "        \n",
    "        trace_update(SpikingNeuron ,input, spike, tau_pre, tau_post)# todo: 需要考察一下trace_pre到底是linuear层的输出还是lif层的输入 fc.trace\n",
    "        ctx.save_for_backward(fc.weight.data,input, spike,SpikingNeuron.trace_pre,SpikingNeuron.trace_post)#\n",
    "\n",
    "        ctx.w_min = w_min\n",
    "        ctx.w_max = w_max\n",
    "        ctx.f_pre = f_pre\n",
    "        ctx.f_post = f_post\n",
    "\n",
    "        return spike\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        weight,input, output,trace_pre,trace_post = ctx.saved_tensors\n",
    "        # grad_input = grad_output.clone()\n",
    "        # grad_output.shape=[200, 10], stdp_grad.shape=[10, 784]\n",
    "        # print(grad_output.shape, stdp_grad(weight, input, output,trace_pre, trace_post, ctx.w_min, ctx.w_max, ctx.f_pre, ctx.f_post).shape)\n",
    "        # 此处返回梯度维度应该为[200, 784]，与输入维度一致,None,None,None,None,None,None,None,None\n",
    "        return torch.mm(grad_output,stdp_grad(weight, input, output,trace_pre, trace_post, ctx.w_min, ctx.w_max, ctx.f_pre, ctx.f_post))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于STDP，当使用trace的时候，需要记录前后神经元的信息，主要需要tau的值，因此重构Linear类，加入tau属性。\n",
    "\n",
    "同时需要考察一下trace_pre到底是linuear层的输出还是lif层的输入，此处作为linear层的输出，加入该属性，方便trace_update函数的调用。\n",
    "\n",
    "在spikingjelly中，此部分代码为：\n",
    "```python\n",
    "class STDPLearner(base.MemoryModule):\n",
    "    def __init__(\n",
    "        self, step_mode: str,\n",
    "        synapse: Union[nn.Conv2d, nn.Linear], sn: neuron.BaseNode,\n",
    "        ...\n",
    "        self.in_spike_monitor = monitor.InputMonitor(synapse)\n",
    "        self.out_spike_monitor = monitor.OutputMonitor(sn)\n",
    "\n",
    "def stdp_linear_single_step(\n",
    "    fc: nn.Linear, in_spike: torch.Tensor, out_spike: torch.Tensor,\n",
    "    trace_pre: Union[float, torch.Tensor, None],\n",
    "    trace_post: Union[float, torch.Tensor, None],\n",
    "    ...\n",
    "    trace_pre = trace_pre - trace_pre / tau_pre + in_spike      # shape = [batch_size, N_in]\n",
    "    trace_post = trace_post - trace_post / tau_post + out_spike # shape = [batch_size, N_out]\n",
    "```\n",
    "因此他认为，trace_pre是linear层的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当使用`loss.backward()`时，需要计算全连接层权重的梯度，即计算\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w}=\\frac{\\partial L}{\\partial z}\\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial w}\n",
    "$$\n",
    "其中，$z$为LIF模型输出，$y$为全连接层输出。\n",
    "$\\frac{\\partial L}{\\partial z}$为自定义反向传播的grad_output，自定义的反向传播值为$\\frac{\\partial y}{\\partial z}$.\n",
    "\n",
    "对于nn.Linear来说，$y=X\\cdot W^T$，其中，$X$为$m\\times n$的矩阵，$W$为$p\\times n$的矩阵，$y$为$m\\times p$的矩阵。\n",
    "\n",
    "则$\\frac{\\partial y}{\\partial w}$为$m\\times p$的矩阵，其每个元素为$\\frac{\\partial y_{ij}}{\\partial w_{kl}}$，其中，$i$为行，$j$为列，$k$为行，$l$为列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Spiking(nn.Module):\n",
    "    def __init__(self, in_feature, out_feature, f_pre, f_post, tau_pre=2.0, tau_post=2.0, v_threshold=1.0, v_reset=0.0, soft_reset=True, w_min=-1.0, w_max=1.0):\n",
    "        super(Linear_Spiking, self).__init__()\n",
    "        self.tau_pre = tau_pre\n",
    "        self.tau_post = tau_post\n",
    "        self.v = None\n",
    "        self.s = None\n",
    "        self.v_threshold = v_threshold\n",
    "        self.v_reset = v_reset\n",
    "        self.soft_reset = soft_reset\n",
    "        self.trace_pre = None\n",
    "        self.trace_post = None\n",
    "        self.w_min = w_min\n",
    "        self.w_max = w_max\n",
    "        self.f_pre = f_pre\n",
    "        self.f_post = f_post\n",
    "        self.linear = nn.Linear(in_feature, out_feature, bias=False)\n",
    "        # self.lif=LIFNeuron(tau=tau, v_threshold=v_threshold, v_reset=v_reset, soft_reset=soft_reset)\n",
    "\n",
    "    def reset(self):\n",
    "        self.v = None\n",
    "        self.s = None\n",
    "        self.trace_pre = None\n",
    "        self.trace_post = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.linear.weight.data.clamp_(self.w_min, self.w_max)\n",
    "        # (ctx,input,fc,SpikingNeuron,tau_pre,tau_post, w_min, w_max, f_pre, f_post)\n",
    "        spike = ActFun.apply(x, self.linear, self, self.tau_pre,\n",
    "                             self.tau_post, self.w_min, self.w_max, self.f_pre, self.f_post)\n",
    "        # spike=(torch.rand(200,10)>0.7).to(args.device)\n",
    "\n",
    "        return spike\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    Linear_Spiking(784,10,f_pre, f_post)\n",
    ")\n",
    "# if args.opt == 'adam':\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "model=model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.1000, Train Acc: 0.0987, Time: 10.8497\n",
      "Epoch: 1, Train Loss: 0.1000, Train Acc: 0.0987, Time: 14.3037\n",
      "Epoch: 2, Train Loss: 0.1000, Train Acc: 0.0987, Time: 11.4937\n",
      "Epoch: 3, Train Loss: 0.1000, Train Acc: 0.0987, Time: 11.0757\n",
      "Epoch: 4, Train Loss: 0.1000, Train Acc: 0.0987, Time: 10.7054\n"
     ]
    }
   ],
   "source": [
    "acc_max = 0\n",
    "epoch_max = 0\n",
    "for epoch in range(args.epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    model.train()\n",
    "    for imag, label in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        imag = imag.to(args.device)\n",
    "        label = label.to(args.device)\n",
    "        label_onehot = F.one_hot(label, 10).float().to(args.device)\n",
    "        out_fr = 0.\n",
    "        for t in range(args.T):\n",
    "            imag_possion = Possion_Encoder(imag)\n",
    "            out_fr += model(imag_possion)\n",
    "            # model.net[1].trace_pre, model.net[1].trece_post, grad = stdp_linear_single_step(\n",
    "            #     model.net[0], imag_possion, out_fr, model.net[1].trace_pre, model.net[1].trace_post, 1, 1, 0, 1, f_pre, f_post)\n",
    "            # model.net[0].weight.grad += grad\n",
    "\n",
    "        out_fr = out_fr / args.T\n",
    "\n",
    "        out_fr.requires_grad = True\n",
    "\n",
    "        loss = F.mse_loss(out_fr, label_onehot)\n",
    "        # model.net[1].trace_pre, model.net[1].trece_post, model.net[0].weight.grad = stdp_linear_single_step(\n",
    "        #     model.net[0], imag_possion, out_fr, model.net[1].trace_pre, model.net[1].trace_post, 1, 1, 0, 1, f_pre, f_post)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "    print('Epoch: {}, Train Loss: {:.4f}, Train Acc: {:.4f}, Time: {:.4f}'.format(\n",
    "        epoch, train_loss / train_samples, train_acc / train_samples, time.time() - start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51582ed516be4194d0681ab41331bdf49015ae552f1f95522fcb6a8efc2a378b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

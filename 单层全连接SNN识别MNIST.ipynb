{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.cuda import amp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from spikingjelly.activation_based import neuron, encoding, functional, surrogate, layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self, tau):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            layer.Flatten(),\n",
    "            layer.Linear(28 * 28, 10, bias=False),\n",
    "            neuron.LIFNode(tau=tau, surrogate_function=surrogate.ATan()),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(T=100, amp=True, b=64, data_dir='./datasets', device='cuda:0', epochs=1, j=8, lr=0.001, momentum=0.9, opt='adam', resume=False, tau=2.0)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='LIF MNIST Training')\n",
    "parser.add_argument('-T', default=100, type=int, help='simulating time-steps')\n",
    "parser.add_argument('-device', default='cuda:0', help='device')\n",
    "parser.add_argument('-b', default=64, type=int, help='batch size')\n",
    "parser.add_argument('-epochs', default=1, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('-j', default=8, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('-data-dir', default='./datasets', type=str, help='root dir of MNIST dataset')\n",
    "parser.add_argument('-resume', default=False, type=str, help='resume from the checkpoint path')\n",
    "parser.add_argument('-amp', default=True,action='store_true', help='automatic mixed precision training')\n",
    "parser.add_argument('-opt', type=str, choices=['sgd', 'adam'], default='adam', help='use which optimizer. SGD or Adam')\n",
    "parser.add_argument('-momentum', default=0.9, type=float, help='momentum for SGD')\n",
    "parser.add_argument('-lr', default=1e-3, type=float, help='learning rate')\n",
    "parser.add_argument('-tau', default=2.0, type=float, help='parameter tau of LIF neuron')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "print(args)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNN(\n",
      "  (layer): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1, step_mode=s)\n",
      "    (1): Linear(in_features=784, out_features=10, bias=False)\n",
      "    (2): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SNN(\n",
       "  (layer): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1, step_mode=s)\n",
       "    (1): Linear(in_features=784, out_features=10, bias=False)\n",
       "    (2): LIFNode(\n",
       "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=s, backend=torch, tau=2.0\n",
       "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = SNN(tau=args.tau)\n",
    "\n",
    "print(net)\n",
    "net.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化数据加载器\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=args.data_dir,\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=args.data_dir,\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "train_data_loader = data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=args.b,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=args.j,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_data_loader = data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=args.b,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=args.j,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None\n",
    "if args.amp:\n",
    "    scaler = amp.GradScaler()\n",
    "\n",
    "start_epoch = 0\n",
    "max_test_acc = -1\n",
    "\n",
    "optimizer = None\n",
    "if args.opt == 'sgd':\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "elif args.opt == 'adam':\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "else:\n",
    "    raise NotImplementedError(args.opt)\n",
    "\n",
    "if args.resume:\n",
    "    checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "    net.load_state_dict(checkpoint['net'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    max_test_acc = checkpoint['max_test_acc']\n",
    "\n",
    "\n",
    "encoder = encoding.PoissonEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(T=100, amp=True, b=64, data_dir='./datasets', device='cuda:0', epochs=1, j=8, lr=0.001, momentum=0.9, opt='adam', resume=False, tau=2.0)\n",
      "epoch =0, train_loss = 0.0273, train_acc = 0.8701, test_loss = 0.0192, test_acc = 0.9117, max_test_acc = 0.9117\n",
      "train speed = 756.2034 images/s, test speed = 2408.4661 images/s\n",
      "escape time = 2022-11-25 23:29:19\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start_epoch, args.epochs):\n",
    "    start_time = time.time()\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for img, label in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        img = img.to(args.device)\n",
    "        label = label.to(args.device)\n",
    "        label_onehot = F.one_hot(label, 10).float()\n",
    "\n",
    "        if scaler is not None:\n",
    "            with amp.autocast():\n",
    "                out_fr = 0.\n",
    "                for t in range(args.T):\n",
    "                    encoded_img = encoder(img)\n",
    "                    out_fr += net(encoded_img)\n",
    "                out_fr = out_fr / args.T\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out_fr = 0.\n",
    "            for t in range(args.T):\n",
    "                encoded_img = encoder(img)\n",
    "                out_fr += net(encoded_img)\n",
    "            out_fr = out_fr / args.T\n",
    "            loss = F.mse_loss(out_fr, label_onehot)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "        # print('label: ', label)\n",
    "        # print('loss: ', loss.item())\n",
    "        # print('train_loss: ', train_loss)\n",
    "        # print('train_acc: ', train_acc)\n",
    "        # label:  tensor([5, 9, 8, 9, 5, 5, 7, 0, 8, 0, 7, 1, 0, 1, 6, 0, 3, 1, 3, 3, 7, 2, 1, 6,\n",
    "        # 8, 9, 2, 1, 8, 8, 1, 8, 1, 0, 0, 5, 0, 7, 2, 1, 4, 6, 5, 1, 2, 1, 9, 3,\n",
    "        # 8, 4, 7, 3, 5, 0, 2, 3, 2, 1, 0, 1, 9, 5, 2, 9], device='cuda:0')\n",
    "        # loss:  0.10000000149011612\n",
    "        # train_loss:  6.400000095367432\n",
    "        # train_acc:  9.0\n",
    "\n",
    "        functional.reset_net(net)\n",
    "\n",
    "    train_time = time.time()\n",
    "    train_speed = train_samples / (train_time - start_time)\n",
    "    train_loss /= train_samples\n",
    "    train_acc /= train_samples\n",
    "\n",
    "\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    test_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for img, label in test_data_loader:\n",
    "            img = img.to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            label_onehot = F.one_hot(label, 10).float()\n",
    "            out_fr = 0.\n",
    "            for t in range(args.T):\n",
    "                encoded_img = encoder(img)\n",
    "                out_fr += net(encoded_img)\n",
    "            out_fr = out_fr / args.T\n",
    "            loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "            test_samples += label.numel()\n",
    "            test_loss += loss.item() * label.numel()\n",
    "            test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "            functional.reset_net(net)\n",
    "    test_time = time.time()\n",
    "    test_speed = test_samples / (test_time - train_time)\n",
    "    test_loss /= test_samples\n",
    "    test_acc /= test_samples\n",
    "\n",
    "    save_max = False\n",
    "    if test_acc > max_test_acc:\n",
    "        max_test_acc = test_acc\n",
    "        save_max = True\n",
    "\n",
    "    checkpoint = {\n",
    "        'net': net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'max_test_acc': max_test_acc\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    print(args)\n",
    "    print(f'epoch ={epoch}, train_loss ={train_loss: .4f}, train_acc ={train_acc: .4f}, test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}, max_test_acc ={max_test_acc: .4f}')\n",
    "    print(f'train speed ={train_speed: .4f} images/s, test speed ={test_speed: .4f} images/s')\n",
    "    print(f'escape time = {(datetime.datetime.now() + datetime.timedelta(seconds=(time.time() - start_time) * (args.epochs - epoch))).strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firing rate: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "v_t_array: [[-0.29920655 -2.1234877  -0.34667656  0.27111456 -0.91487545 -0.5903152\n",
      "  -1.081855    0.         -0.12994415 -0.09245802]\n",
      " [-0.56135046 -3.2449474  -0.5342432   0.32430953 -1.3813639  -0.7676717\n",
      "  -1.7292999   0.         -0.2601941  -0.07444678]\n",
      " [-0.42563343 -3.3545718  -0.47962427  0.31095773 -1.5317702  -0.7029953\n",
      "  -1.8540742   0.         -0.38154185 -0.16001144]\n",
      " [-0.45681864 -3.6512332  -0.3617103   0.46627215 -1.4908309  -0.904159\n",
      "  -2.1455832   0.         -0.39970565 -0.2900252 ]\n",
      " [-0.5011374  -3.6133394  -0.4066478   0.27096352 -1.6788766  -0.6792593\n",
      "  -2.170682    0.         -0.43814945 -0.42537618]\n",
      " [-0.61798954 -3.5834594  -0.4156293   0.47524625 -1.7780392  -0.8914727\n",
      "  -2.3014255   0.         -0.40963766 -0.28636733]\n",
      " [-0.4824766  -3.4112496  -0.39720678  0.2831207  -2.0067127  -0.8851657\n",
      "  -2.1426694   0.         -0.54436094 -0.22954482]\n",
      " [-0.5222386  -3.540553   -0.4803449   0.23832893 -1.9021075  -0.8203033\n",
      "  -2.0942569   0.         -0.31301367 -0.20268205]\n",
      " [-0.47042847 -3.7708158  -0.4792822   0.2337156  -1.6495268  -0.87884915\n",
      "  -2.0935316   0.         -0.19976878 -0.2778076 ]\n",
      " [-0.5189301  -3.6556458  -0.49301463  0.5387515  -1.9215227  -0.82594895\n",
      "  -2.2868702   0.         -0.45830163 -0.3113711 ]\n",
      " [-0.5885941  -3.8248992  -0.53394043  0.58124554 -1.9998069  -0.8782301\n",
      "  -2.3793054   0.         -0.404787   -0.26898402]\n",
      " [-0.6629146  -3.6683261  -0.55822647  0.45272642 -2.1070418  -0.8119546\n",
      "  -2.222558    0.         -0.54271287 -0.2273783 ]\n",
      " [-0.48673874 -3.6308808  -0.46989053  0.3141893  -1.9844322  -0.7153139\n",
      "  -2.1108198   0.         -0.41768974 -0.06766252]\n",
      " [-0.44730592 -3.7172832  -0.4601704   0.34223616 -1.9243281  -0.6764541\n",
      "  -2.000528    0.         -0.40498883 -0.21604116]\n",
      " [-0.24960537 -3.655073   -0.3115356   0.49555895 -2.0124784  -0.7217984\n",
      "  -2.1283176   0.         -0.4749537  -0.4490918 ]\n",
      " [-0.32911983 -3.7327623  -0.4154019   0.33575916 -1.9690148  -0.7815482\n",
      "  -2.0993562   0.         -0.37257484 -0.333171  ]\n",
      " [-0.37910855 -3.8077924  -0.3729543   0.41401464 -1.5591552  -0.6475303\n",
      "  -2.1125498   0.         -0.27417827 -0.49174038]\n",
      " [-0.25402585 -3.7459106  -0.31377387  0.3484227  -1.7362804  -0.846105\n",
      "  -2.048493    0.         -0.2868216  -0.41849664]\n",
      " [-0.50901806 -3.8422647  -0.47694254  0.40477008 -1.8565342  -0.73176765\n",
      "  -2.0405555   0.         -0.30594242 -0.5105854 ]\n",
      " [-0.6868375  -4.164609   -0.5866464   0.5392331  -1.88304    -0.7987147\n",
      "  -2.2373743   0.         -0.31701392 -0.41734773]\n",
      " [-0.56144166 -3.9859557  -0.6241857   0.5441028  -1.7725301  -0.8409138\n",
      "  -2.2633822   0.         -0.45554516 -0.260653  ]\n",
      " [-0.43548453 -3.9123945  -0.5441544   0.5749823  -1.7954569  -0.81646407\n",
      "  -2.13585     0.         -0.36096382 -0.30634585]\n",
      " [-0.667873   -4.079254   -0.5656357   0.6206672  -1.7834784  -0.8240477\n",
      "  -2.1598487   0.         -0.45083117 -0.41939378]\n",
      " [-0.50771487 -3.9177232  -0.5456328   0.49158585 -1.6816845  -0.72270256\n",
      "  -2.0101216   0.         -0.40990126 -0.36984614]\n",
      " [-0.3783695  -3.5834565  -0.5768338   0.51336145 -1.9074851  -0.5184331\n",
      "  -2.1317058   0.         -0.4375034  -0.4586833 ]\n",
      " [-0.6061554  -3.9338343  -0.65827084  0.47426683 -1.7974154  -0.66644746\n",
      "  -2.3099499   0.         -0.37924418 -0.4687056 ]\n",
      " [-0.6557307  -3.7335157  -0.45360917  0.47059804 -1.6829033  -0.7701374\n",
      "  -2.1467328   0.         -0.4349705  -0.29515404]\n",
      " [-0.76222587 -3.7280097  -0.47280854  0.3081719  -1.6696312  -0.66363484\n",
      "  -1.9993036   0.         -0.46526515 -0.36798245]\n",
      " [-0.6470021  -3.9695456  -0.42111698  0.40085888 -1.7111771  -0.62386966\n",
      "  -1.9913815   0.         -0.44018924 -0.52625906]\n",
      " [-0.6631444  -3.7704926  -0.45997128  0.22263122 -1.5791063  -0.6109782\n",
      "  -1.7176819   0.         -0.3685761  -0.56779665]\n",
      " [-0.7160919  -3.9172182  -0.45884323  0.29721782 -1.5587976  -0.8263806\n",
      "  -1.9531175   0.         -0.37505144 -0.42927477]\n",
      " [-0.9172749  -3.9874659  -0.48202586  0.31443578 -1.4796097  -0.97995716\n",
      "  -2.2131548   0.         -0.4104934  -0.52685004]\n",
      " [-0.6830543  -3.8394532  -0.53755385  0.42690465 -1.7028738  -0.6662851\n",
      "  -2.337626    0.         -0.37053737 -0.5405725 ]\n",
      " [-0.48467305 -3.9112368  -0.5081245   0.50293756 -1.6516254  -0.79490304\n",
      "  -2.153407    0.         -0.39894956 -0.44503254]\n",
      " [-0.5918507  -3.733546   -0.45862663  0.39941773 -1.7565284  -0.6868711\n",
      "  -2.168904    0.         -0.48176944 -0.38951087]\n",
      " [-0.37812072 -4.043687   -0.49442387  0.41536382 -1.9008014  -0.52023554\n",
      "  -2.1401973   0.         -0.25753516 -0.39766717]\n",
      " [-0.4296422  -3.9809566  -0.4809128   0.43934214 -2.1459842  -0.7005022\n",
      "  -2.1085124   0.         -0.3374561  -0.4395764 ]\n",
      " [-0.4652711  -3.9600883  -0.53659046  0.4796864  -2.1737084  -0.8935604\n",
      "  -2.1372283   0.         -0.4549218  -0.2868766 ]\n",
      " [-0.4138     -3.964791   -0.62189114  0.59725755 -2.238232   -0.80924463\n",
      "  -2.2038121   0.         -0.49668145 -0.36456153]\n",
      " [-0.6520034  -4.1226597  -0.6152442   0.48336452 -2.0276465  -0.81288666\n",
      "  -2.1749496   0.         -0.46792087 -0.2661185 ]\n",
      " [-0.41766652 -3.963678   -0.49105695  0.42916673 -1.8981296  -0.8192129\n",
      "  -2.0622642   0.         -0.32021958 -0.24214312]\n",
      " [-0.75398827 -4.0746775  -0.52545196  0.4620676  -1.7145483  -0.7078725\n",
      "  -2.1984959   0.         -0.37113917 -0.36900634]\n",
      " [-0.45546967 -3.9752614  -0.50190353  0.4679819  -1.9297531  -0.5507465\n",
      "  -2.167303    0.         -0.36243984 -0.3661589 ]\n",
      " [-0.38609567 -4.0819206  -0.37006298  0.3696784  -1.7701429  -0.8085829\n",
      "  -2.0915499   0.         -0.3190261  -0.37282455]\n",
      " [-0.53262615 -4.115617   -0.46627903  0.31624365 -1.5902898  -0.7182836\n",
      "  -2.0216238   0.         -0.3289007  -0.38924015]\n",
      " [-0.5680879  -3.8318353  -0.3265959   0.46134248 -1.5996659  -0.55987704\n",
      "  -2.0042777   0.         -0.50052226 -0.49201697]\n",
      " [-0.6925586  -3.84172    -0.36635238  0.32041568 -1.5781349  -0.5841749\n",
      "  -1.950368    0.         -0.3986773  -0.3294083 ]\n",
      " [-0.7840171  -3.9839082  -0.47669578  0.41686064 -1.7974546  -0.74905264\n",
      "  -2.185552    0.         -0.50806046 -0.55024767]\n",
      " [-0.38752645 -3.736012   -0.42062658  0.5261575  -1.80807    -0.83380944\n",
      "  -2.0489697   0.         -0.3995272  -0.35469145]\n",
      " [-0.59727764 -3.8063939  -0.6080462   0.584005   -1.7905422  -0.6478085\n",
      "  -2.2964344   0.         -0.45931083 -0.4288997 ]\n",
      " [-0.44726035 -3.610423   -0.49375877  0.7070893  -1.8140182  -0.7789469\n",
      "  -2.2838087   0.         -0.54478526 -0.3534033 ]\n",
      " [-0.3943563  -3.8003254  -0.4745158   0.62053764 -1.9112643  -0.6605705\n",
      "  -2.4065895   0.         -0.38387182 -0.5218976 ]\n",
      " [-0.47854415 -3.7306094  -0.42892492  0.4994343  -1.6812019  -0.81042796\n",
      "  -2.2595973   0.         -0.4262901  -0.44397026]\n",
      " [-0.7300854  -3.7123373  -0.4823498   0.44536906 -1.8452154  -0.4487686\n",
      "  -2.0698729   0.         -0.5262565  -0.4333695 ]\n",
      " [-0.76983476 -3.797731   -0.47168744  0.4036978  -1.6563926  -0.4689756\n",
      "  -2.0244932   0.         -0.47881716 -0.44551525]\n",
      " [-0.5738822  -3.908225   -0.4233234   0.42922175 -1.6361563  -0.7646173\n",
      "  -2.084866    0.         -0.29236197 -0.31604984]\n",
      " [-0.5691933  -3.802291   -0.45061997  0.3483251  -1.6596212  -0.78922224\n",
      "  -1.9917408   0.         -0.3804011  -0.39949816]\n",
      " [-0.4944948  -3.9784071  -0.5282557   0.4773543  -1.7667451  -0.69858354\n",
      "  -2.2998528   0.         -0.33069444 -0.3789083 ]\n",
      " [-0.60053444 -3.788417   -0.38109642  0.32453874 -1.6974413  -0.5528225\n",
      "  -2.0968194   0.         -0.5013671  -0.40461877]\n",
      " [-0.21179229 -3.6570992  -0.34425336  0.3234583  -1.7054968  -0.4934376\n",
      "  -2.030189    0.         -0.33378622 -0.50568855]\n",
      " [-0.3161167  -3.8607965  -0.43997726  0.52661955 -1.7567791  -0.8206936\n",
      "  -2.1773305   0.         -0.30542132 -0.40121955]\n",
      " [-0.354613   -3.963774   -0.4751007   0.51823664 -1.8209019  -0.8079901\n",
      "  -2.050303    0.         -0.329292   -0.39267576]\n",
      " [-0.69151855 -3.8427157  -0.51519287  0.4015186  -1.4882139  -0.6290458\n",
      "  -1.8248267   0.         -0.48783794 -0.4293939 ]\n",
      " [-0.7083214  -3.865292   -0.4784491   0.41139293 -1.811667   -0.54199135\n",
      "  -1.956256    0.         -0.58453953 -0.4770611 ]\n",
      " [-0.6712034  -3.8348174  -0.4961472   0.42278907 -1.751123   -0.6964389\n",
      "  -1.9765141   0.         -0.57866347 -0.3634677 ]\n",
      " [-0.6444979  -3.8748162  -0.48187268  0.3692543  -1.8134117  -0.75371015\n",
      "  -2.0632043   0.         -0.4286657  -0.31350428]\n",
      " [-0.4561973  -3.7982588  -0.3305202   0.27853557 -1.7879512  -0.6226792\n",
      "  -2.1140563   0.         -0.36753488 -0.3826003 ]\n",
      " [-0.50455314 -3.7976713  -0.3050254   0.3475892  -1.7636995  -0.5992693\n",
      "  -1.8999698   0.         -0.41511124 -0.2851869 ]\n",
      " [-0.6804887  -3.7589588  -0.48335752  0.274175   -1.8083763  -0.5433342\n",
      "  -1.7518548   0.         -0.4776119  -0.18835472]\n",
      " [-0.4140759  -3.6635342  -0.45890707  0.4333965  -1.9417887  -0.63214594\n",
      "  -1.8462353   0.         -0.4733245  -0.27028954]\n",
      " [-0.7135546  -3.9212544  -0.45373774  0.6413596  -1.8815513  -0.64194286\n",
      "  -2.2312775   0.         -0.54798174 -0.35309094]\n",
      " [-0.6288765  -3.5938087  -0.5814503   0.3532674  -1.778144   -0.48353893\n",
      "  -2.0393758   0.         -0.43121648 -0.25264755]\n",
      " [-0.63765395 -3.7844057  -0.6141216   0.45386237 -1.6991479  -0.75728166\n",
      "  -2.1058197   0.         -0.4368625  -0.353774  ]\n",
      " [-0.3989584  -3.6558385  -0.59631157  0.39914262 -1.7170529  -0.8164817\n",
      "  -2.2140274   0.         -0.3477685  -0.42044216]\n",
      " [-0.5293942  -3.9036005  -0.5205779   0.46654433 -1.7039325  -0.8766165\n",
      "  -2.207594    0.         -0.31135324 -0.3542026 ]\n",
      " [-0.64843094 -4.0674872  -0.66872096  0.39603886 -1.9001892  -0.666264\n",
      "  -2.210929    0.         -0.39174968 -0.69833004]\n",
      " [-0.39754432 -3.9253922  -0.49714968  0.4528048  -2.0146298  -0.68777966\n",
      "  -2.1177104   0.         -0.3378262  -0.5545293 ]\n",
      " [-0.45201412 -3.9338555  -0.51986986  0.3970328  -1.8270769  -0.5262481\n",
      "  -2.117322    0.         -0.21577176 -0.5194042 ]\n",
      " [-0.44301564 -3.9450765  -0.5536325   0.5194783  -1.7774727  -0.8376905\n",
      "  -1.924196    0.         -0.21626575 -0.3318271 ]\n",
      " [-0.55050576 -4.139513   -0.5770595   0.5545168  -1.7083006  -0.8122313\n",
      "  -2.1550806   0.         -0.24039239 -0.37192827]\n",
      " [-0.44163975 -3.833466   -0.34855467  0.40018588 -1.6596422  -0.8788064\n",
      "  -2.1706853   0.         -0.14508449 -0.47647297]\n",
      " [-0.56610304 -3.8881032  -0.5179398   0.19968702 -1.6956836  -0.84651065\n",
      "  -1.993309    0.         -0.1755536  -0.28710422]\n",
      " [-0.58000994 -3.8782058  -0.4937802   0.37870264 -1.810184   -0.71148187\n",
      "  -2.125337    0.         -0.3022892  -0.32671186]\n",
      " [-0.44351625 -4.13628    -0.43775922  0.29084694 -1.642358   -0.61076295\n",
      "  -2.1913216   0.         -0.30898744 -0.39938736]\n",
      " [-0.6820406  -4.199111   -0.6362227   0.37513894 -1.6217523  -0.48288536\n",
      "  -2.2515018   0.         -0.27436    -0.38907588]\n",
      " [-0.26605988 -3.8386774  -0.41987434  0.43195522 -1.6955779  -0.42790467\n",
      "  -2.1707397   0.         -0.28137878 -0.44289574]\n",
      " [-0.49393415 -3.773962   -0.4007758   0.41934928 -1.5049905  -0.6561017\n",
      "  -1.940724    0.         -0.39994484 -0.31878042]\n",
      " [-0.41469046 -3.883018   -0.31361264  0.46326858 -1.676377   -0.7637538\n",
      "  -2.124348    0.         -0.26280144 -0.30104005]\n",
      " [-0.32430875 -3.8822896  -0.41633603  0.5257343  -1.602963   -0.6919519\n",
      "  -2.2264423   0.         -0.2525565  -0.31841415]\n",
      " [-0.52997327 -3.8422008  -0.49961412  0.49746913 -1.8065325  -0.6819569\n",
      "  -2.0885634   0.         -0.34062952 -0.34056854]\n",
      " [-0.5058478  -3.8564916  -0.5050421   0.5317795  -1.9090858  -0.7495939\n",
      "  -2.1312792   0.         -0.43248704 -0.23646586]\n",
      " [-0.5096383  -3.8334253  -0.5530303   0.72260517 -1.9073486  -0.56279564\n",
      "  -2.3301115   0.         -0.3826301  -0.29530665]\n",
      " [-0.37961906 -3.822782   -0.33447704  0.45905343 -1.664313   -0.6701622\n",
      "  -2.0049288   0.         -0.3355369  -0.48969918]\n",
      " [-0.43001804 -3.7539883  -0.29655302  0.54055554 -1.7939711  -0.76796246\n",
      "  -2.2231336   0.         -0.4079716  -0.3711892 ]\n",
      " [-0.438798   -3.636134   -0.2987799   0.5204289  -1.846142   -0.6440403\n",
      "  -2.2431893   0.         -0.4441511  -0.44845134]\n",
      " [-0.69526803 -3.7857494  -0.43979114  0.28404835 -1.8403536  -0.6046878\n",
      "  -1.9974926   0.         -0.62506217 -0.4035644 ]\n",
      " [-0.773451   -3.7607336  -0.48866352  0.25826222 -1.7505436  -0.48416704\n",
      "  -2.0372      0.         -0.4652629  -0.47373724]\n",
      " [-0.61877054 -3.9861917  -0.55119896  0.3227656  -1.9236654  -0.44749394\n",
      "  -2.0498662   0.         -0.41659027 -0.45081437]\n",
      " [-0.5850105  -4.010782   -0.4262064   0.31017715 -1.9323134  -0.63718396\n",
      "  -2.0411787   0.         -0.4502388  -0.4237699 ]\n",
      " [-0.56134236 -4.1189804  -0.42852497  0.56709653 -1.7863942  -0.86659646\n",
      "  -2.1962705   0.         -0.51166207 -0.3227488 ]]\n",
      "s_t_array: [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 保存绘图用数据\n",
    "net.eval()\n",
    "# 注册钩子\n",
    "output_layer = net.layer[-1] # 输出层\n",
    "output_layer.v_seq = []\n",
    "output_layer.s_seq = []\n",
    "def save_hook(m, x, y):\n",
    "    m.v_seq.append(m.v.unsqueeze(0))\n",
    "    m.s_seq.append(y.unsqueeze(0))\n",
    "\n",
    "output_layer.register_forward_hook(save_hook)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    img, label = test_dataset[0]\n",
    "    img = img.to(args.device)\n",
    "    out_fr = 0.\n",
    "    for t in range(args.T):\n",
    "        encoded_img = encoder(img)\n",
    "        out_fr += net(encoded_img)\n",
    "    out_spikes_counter_frequency = (out_fr / args.T).cpu().numpy()\n",
    "    print(f'Firing rate: {out_spikes_counter_frequency}')\n",
    "\n",
    "    output_layer.v_seq = torch.cat(output_layer.v_seq)\n",
    "    output_layer.s_seq = torch.cat(output_layer.s_seq)\n",
    "    v_t_array = output_layer.v_seq.cpu().numpy().squeeze()  # v_t_array[i][j]表示神经元i在j时刻的电压值\n",
    "    print('v_t_array:',v_t_array)\n",
    "    s_t_array = output_layer.s_seq.cpu().numpy().squeeze()  # s_t_array[i][j]表示神经元i在j时刻释放的脉冲，为0或1\n",
    "    print('s_t_array:',s_t_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51582ed516be4194d0681ab41331bdf49015ae552f1f95522fcb6a8efc2a378b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

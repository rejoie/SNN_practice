## 一、基本概念

1. 数据格式：

   - 表示单个时刻的数据，其 shape = [N, \*]，其中 N 是batch维度，\* 表示任意额外的维度
   - 表示多个时刻的数据，其 shape = [T, N, \*]，其中 T 是数据的时间维度， N 是batch维度，\* 表示任意额外的维度
2. 步进模式：单步模式(single-step)和多步模式(multi-step)。

   - 在单步模式下，数据使用 `shape = [N, *]`的格式；而在多步模式下，数据使用 `shape = [T, N, *]`的格式。模块在初始化时可以指定其使用的步进模式 step_mode，也可以在构建后直接进行修改。
   - shape = [T, N, *]的序列数据，通常需要手动做一个时间上的循环，将数据拆成T个shape = [N, *]的数据并逐步输入进去。
   - multi_step_forward提供了将 shape = [T, N, *] 的序列数据输入到单步模块进行逐步的前向传播的封装，直接将模块设置成多步模块，更为便捷。
   - 默认为单步。
3. 状态的保存和重置：状态会被保存在模块内部，初始化后，IF神经元层的 v 会被设置为0，首次给与输入后 v 会自动广播到与输入相同的 shape。若需要一个新的输入，则应该先清除神经元之前的状态，可以通过调用模块的 self.reset() 函数实现。可以通过调用 spikingjelly.activation_based.functional.reset_net 将整个网络中的所有有状态模块进行重置。

   - 若网络使用了有状态的模块，在训练和推理时，务必在处理完毕一个batch的数据后进行重置。
4. 传播模式：逐步传播(step-by-step)和逐层传播(layer-by-layer)，实际上只是计算顺序不同，它们的计算结果是完全相同。

   - 在使用梯度替代法训练时，通常推荐使用逐层传播。在正确构建网络的情况下，逐层传播的并行度更大，速度更快。
   - 在内存受限时使用逐步传播，例如ANN2SNN任务中需要用到非常大的 T。因为在逐层传播模式下，对无状态的层而言，真正的 batch size 是 TN 而不是 N，当 T 太大时内存消耗极大。

## 二、包装器

1. multi_step_forward 可以将一个单步模块进行多步传播
2. MultiStepContainer 可以将一个单步模块包装成多步模块
3. ANN的网络层本身是无状态的，不存在前序依赖，没有必要在时间上串行的计算，可以使用函数风格的 seq_to_ann_forward 或模块风格的 SeqToANNContainer 进行包装。

   - seq_to_ann_forward 将 shape = [T, N, *] 的数据首先变换为 shape = [TN, *]，再送入无状态的网络层进行计算，输出的结果会被重新变换为 shape = [T, N, *]。不同时刻的数据是并行计算的
4. spikingjelly.activation_based.layer中已定义常用的网络层，相比手动包装，存在以下优势：

   - 支持单步和多步模式，而 SeqToANNContainer 和 MultiStepContainer 包装的层，只支持多步模式
   - 包装器会使得 state_dict 的 keys() 也增加一层包装，给加载权重带来麻烦
5. MultiStepContainer 和 SeqToANNContainer 都是只支持多步模式的，不允许切换为单步模式。
6. StepModeContainer 类似于融合版的 MultiStepContainer 和 SeqToANNContainer，可以用于包装无状态或有状态的单步模块，需要在包装时指明是否有状态，但此包装器还支持切换单步和多步模式。
7. 使用 set_step_mode 改变 StepModeContainer只会改变包装器本身的 step_mode，而包装器内的模块仍然保持单步
8. 如果模块本身就支持单步和多步模式的切换，则不推荐使用 MultiStepContainer 或 StepModeContainer 对其进行包装。因为包装器使用的多步前向传播，可能不如模块自身定义的前向传播速度快。
9. 通常需要用到 MultiStepContainer 或 StepModeContainer 的是一些没有定义多步的模块，例如一个在 torch.nn 中存在，但在 spikingjelly.activation_based.layer 中不存在的网络层。

## 三、神经元

1. spikingjelly.activation_based.neuron 中的神经元，在构造函数的参数之一 v_reset，默认为 1.0 ，表示神经元使用Hard方式；若设置为 None，则会使用Soft方式。
2. 可以用充电、放电、重置，这3个离散方程来描述任意的离散脉冲神经元。充电、放电方程为：

   $$
   H[t]=f(V[t-1],X[t])\\
   S[t]=\Theta(H(t)-V_{threshold})
   $$

   其中$\Theta(x)$即为构造函数参数中的 surrogate_function，是一个阶跃函数：

   $$
   \Theta(x)=1,x\geq0,\Theta(x)=0,x<0
   $$

   Hard方式重置方程为：

   $$
   V[t]=H[t](1-S[t])+V_{reset}S[t]
   $$

   Soft方式重置方程为：

   $$
   V[t]=H[t]-V_{threshold}S[t]
   $$
3. 单步模式下的前向传播 single_step_forward 函数由充电、放电、重置三个过程组成:

   ```python
   # spikingjelly.activation_based.neuron.BaseNode
   def single_step_forward(self, x: torch.Tensor):
       self.neuronal_charge(x)
       spike = self.neuronal_fire()
       self.neuronal_reset(spike)
       return spike
   ```
   其中 neuronal_fire 和 neuronal_reset 对绝大多数神经元都是相同的，因而在 BaseNode 中就已经定义了。若想实现新的神经元，则只需要更改构造函数和充电方程即可。
4. BaseNode 继承自 MemoryModule。使用 for t in range(T) 来循环调用单步传播实现多步传播

## 四、梯度替代

## 五、监视器
1. spikingjelly.activation_based.monitor.OutputMonitor 可以记录网络中任何类型为 instance 的模块的输出。要记录的数据，会根据生成顺序，保存在 .records 的 list 中
2. .monitored_layers 记录了被监视器监控的层的名字，可以直接通过层的名字作为索引，访问某一层被记录的数据。
3. 记录模块的成员变量，可以通过 spikingjelly.activation_based.monitor.AttributeMonitor 实现
4. 设置输入监视器的方法，和设置输出监视器的如出一辙
5. 记录每一层脉冲神经元的输入梯度，可以使用 spikingjelly.activation_based.monitor.GradInputMonitor
6. 记录模块的输出梯度可以使用 spikingjelly.activation_based.monitor.GradOutputMonitor 实现

## 六、数据集
spikingjelly.datasets 中集成了常用的神经形态数据集。
1. 使用参数 data_type='event' 获得event数据集。运行完成后，同级目录下生成events_np 文件夹，包含训练集和测试集。event 使用字典格式存储Events数据，键为 ['t', 'x', 'y', 'p']；label 是数据的标签。
2. 将原始的Event流积分成Frame数据，将原始的Event数据记为$E(x_i,y_i,t_i,p_i),0\le i < N$；设置 split_by='number' 表示从Event数量  上进行划分，接近均匀地划分为 frames_num=20， 也就是T段。记积分后的Frame数据中的某一帧为$F(j)$，在$(p,x,y)$位置的像素值为$F(j,p,x,y)$；$F(j)$是从Event流中索引介于$j_l$和$j_r$的Event 积分而来：
   $$
   j_l=\lfloor \frac{N}{T} \rfloor\times j\\
   j_r=\lfloor \frac{N}{T} \rfloor\times (j+1),j<T-1\\
   j_r=N,j=T-1\\
   F(j,p,x,y)=\sum_{i=j_l}^{j_r-1}I_{p,x,y}(p_i,x_i,y_i)
   $$

3. 使用固定时间间隔积分，更符合实际物理系统。例如每 10 ms 积分一次，则长度为 L ms 的数据，可以得到 math.floor(L / 10) 帧。
4. spikingjelly.datasets.pad_sequence_collate 和 spikingjelly.datasets.padded_sequence_mask 可以很方便的对不等长数据进行对齐和还原。
5. 自定义积分方法需要提供积分函数 custom_integrate_function 以及保存frames的文件夹名 custom_integrated_frames_dir_name。
   - custom_integrate_function 是用户定义的函数，输入是 events, H, W，其中 events 是一个pythono字典，键为 ['t', 'x', 'y', 'p'] 值为 numpy.ndarray 类型。H 是数据高度，W 是数据宽度。
   - custom_integrated_frames_dir_name 可以为 None，在这种情况下，保存frames的文件夹名会被设置成 `custom_integrate_function.__name__`。

# 七、自连接与有状态突触
1. ElementWiseRecurrentContainer 是一个包装器，给任意的 sub_module 增加一个额外的自连接。连接的形式可以使用用户自定义的逐元素函数操作$z=f(x,y)$来实现。
2. 使用 spikingjelly.activation_based.layer.LinearRecurrentContainer 可以实现更复杂的全连接形式的自连接。
3. spikingjelly.activation_based.layer.SynapseFilter 放在普通无状 态突触的后面，对突触输出的电流进行滤波，就可以得到有状态的突触